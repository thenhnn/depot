diff --git c/Cargo.lock w/Cargo.lock
index d89bfe8f..a04dc7d3 100644
--- c/Cargo.lock
+++ w/Cargo.lock
@@ -1179,6 +1179,7 @@ dependencies = [
  "http-body-util",
  "hyper",
  "hyper-util",
+ "listenfd",
  "log",
  "ruma",
  "rustls",
@@ -2890,6 +2891,17 @@ version = "0.11.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "df1d3c3b53da64cf5760482273a98e575c651a67eec7f77df96b5b642de8f039"
 
+[[package]]
+name = "listenfd"
+version = "1.0.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "b87bc54a4629b4294d0b3ef041b64c40c611097a677d9dc07b2c67739fe39dba"
+dependencies = [
+ "libc",
+ "uuid",
+ "winapi",
+]
+
 [[package]]
 name = "litemap"
 version = "0.8.1"
diff --git c/Cargo.toml w/Cargo.toml
index 38def82f..9eddc961 100644
--- c/Cargo.toml
+++ w/Cargo.toml
@@ -548,6 +548,9 @@ features = ["sync", "tls-rustls", "rustls-provider"]
 [workspace.dependencies.resolv-conf]
 version = "0.7.5"
 
+[workspace.dependencies.listenfd]
+version = "1.0.2"
+
 #
 # Patches
 #
diff --git c/src/router/Cargo.toml w/src/router/Cargo.toml
index f25eba57..2e61e795 100644
--- c/src/router/Cargo.toml
+++ w/src/router/Cargo.toml
@@ -122,6 +122,7 @@ tokio.workspace = true
 tower.workspace = true
 tower-http.workspace = true
 tracing.workspace = true
+listenfd.workspace = true
 
 [target.'cfg(all(unix, target_os = "linux"))'.dependencies]
 sd-notify.workspace = true
diff --git c/src/router/serve/mod.rs w/src/router/serve/mod.rs
index 2399edf0..7a416743 100644
--- c/src/router/serve/mod.rs
+++ w/src/router/serve/mod.rs
@@ -1,4 +1,5 @@
 mod plain;
+mod systemd;
 #[cfg(feature = "direct_tls")]
 mod tls;
 mod unix;
@@ -6,7 +7,7 @@
 use std::sync::Arc;
 
 use axum_server::Handle as ServerHandle;
-use conduwuit::{Result, err};
+use conduwuit::{Result, err, info};
 use conduwuit_service::Services;
 use tokio::sync::broadcast;
 
@@ -27,9 +28,14 @@ pub(super) async fn serve(
 			.map_err(|e| err!(error!("channel error: {e}")));
 	}
 
+	let mut listenfd = listenfd::ListenFd::from_env();
+
 	let addrs = config.get_bind_addrs();
 	let (app, _guard) = layers::build(&services)?;
-	if cfg!(unix) && config.unix_socket_path.is_some() {
+	if let Some(std_listener) = listenfd.take_unix_listener(0).unwrap() {
+		info!("Listening on systemd provided socket");
+		systemd::serve(std_listener, server, app, shutdown).await
+	} else if cfg!(unix) && config.unix_socket_path.is_some() {
 		unix::serve(server, app, shutdown).await
 	} else if config.tls.certs.is_some() {
 		#[cfg(feature = "direct_tls")]
diff --git c/src/router/serve/systemd.rs w/src/router/serve/systemd.rs
new file mode 100644
index 00000000..850c5493
--- /dev/null
+++ w/src/router/serve/systemd.rs
@@ -0,0 +1,135 @@
+#![cfg(unix)]
+
+use std::{
+	net::{self, IpAddr, Ipv4Addr},
+	os::fd::AsRawFd,
+	sync::{Arc, atomic::Ordering},
+};
+
+use axum::{
+	Router,
+	extract::{Request, connect_info::IntoMakeServiceWithConnectInfo},
+};
+use conduwuit::{
+	Result, Server, debug, debug_error, result::UnwrapInfallible, trace,
+};
+use hyper::{body::Incoming, service::service_fn};
+use hyper_util::{
+	rt::{TokioExecutor, TokioIo},
+	server,
+};
+use tokio::{
+	net::{UnixListener, UnixStream, unix::SocketAddr},
+	sync::broadcast::{self},
+	task::JoinSet,
+	time::{Duration, sleep},
+};
+use tower::{Service, ServiceExt};
+
+type MakeService = IntoMakeServiceWithConnectInfo<Router, net::SocketAddr>;
+
+const NULL_ADDR: net::SocketAddr = net::SocketAddr::new(IpAddr::V4(Ipv4Addr::new(0, 0, 0, 0)), 0);
+const FINI_POLL_INTERVAL: Duration = Duration::from_millis(750);
+
+#[tracing::instrument(skip_all, level = "debug")]
+pub(super) async fn serve(
+	listener: std::os::unix::net::UnixListener,
+	server: &Arc<Server>,
+	app: Router,
+	mut shutdown: broadcast::Receiver<()>,
+) -> Result<()> {
+	let mut tasks = JoinSet::<()>::new();
+	let executor = TokioExecutor::new();
+	let app = app.into_make_service_with_connect_info::<net::SocketAddr>();
+	let builder = server::conn::auto::Builder::new(executor);
+	let listener = UnixListener::from_std(listener).unwrap();
+	while server.running() {
+		let app = app.clone();
+		let builder = builder.clone();
+		tokio::select! {
+			_sig = shutdown.recv() => break,
+			conn = listener.accept() => match conn {
+				Ok(conn) => accept(server, &listener, &mut tasks, app, builder, conn).await,
+				Err(err) => debug_error!(?listener, "accept error: {err}"),
+			},
+		}
+	}
+
+	fini(server, listener, tasks).await;
+
+	Ok(())
+}
+
+#[tracing::instrument(
+	level = "trace",
+	skip_all,
+	fields(
+		?listener,
+		socket = ?conn.0,
+	),
+)]
+async fn accept(
+	server: &Arc<Server>,
+	listener: &UnixListener,
+	tasks: &mut JoinSet<()>,
+	app: MakeService,
+	builder: server::conn::auto::Builder<TokioExecutor>,
+	conn: (UnixStream, SocketAddr),
+) {
+	let (socket, _) = conn;
+	let server_ = server.clone();
+	let task = async move { accepted(server_, builder, socket, app).await };
+
+	_ = tasks.spawn_on(task, server.runtime());
+	while tasks.try_join_next().is_some() {}
+}
+
+#[tracing::instrument(
+	level = "trace",
+	skip_all,
+	fields(
+		fd = %socket.as_raw_fd(),
+		path = ?socket.local_addr(),
+	),
+)]
+async fn accepted(
+	server: Arc<Server>,
+	builder: server::conn::auto::Builder<TokioExecutor>,
+	socket: UnixStream,
+	mut app: MakeService,
+) {
+	let socket = TokioIo::new(socket);
+	let called = app.call(NULL_ADDR).await.unwrap_infallible();
+	let service = move |req: Request<Incoming>| called.clone().oneshot(req);
+	let handler = service_fn(service);
+	trace!(?socket, ?handler, "serving connection");
+
+	// bug on darwin causes all results to be errors. do not unwrap this
+	tokio::select! {
+		() = server.until_shutdown() => (),
+		_ = builder.serve_connection(socket, handler) => (),
+	};
+}
+
+async fn fini(server: &Arc<Server>, listener: UnixListener, mut tasks: JoinSet<()>) {
+	let local = listener.local_addr();
+
+	debug!("Closing listener at {local:?} ...");
+	drop(listener);
+
+	debug!("Waiting for requests to finish...");
+	while server
+		.metrics
+		.requests_handle_active
+		.load(Ordering::Relaxed)
+		.gt(&0)
+	{
+		tokio::select! {
+			task = tasks.join_next() => if task.is_none() { break; },
+			() = sleep(FINI_POLL_INTERVAL) => {},
+		}
+	}
+
+	debug!("Shutting down...");
+	tasks.shutdown().await;
+}
